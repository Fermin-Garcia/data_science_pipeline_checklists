## Data Acquisition

**1. Identify Data Sources**
- Determine where your data is coming from. This could be internal databases, APIs, web scraping, third-party data providers, etc.
- Be aware of the legal and ethical considerations when collecting data from any source.

**2. SQL Databases**
- SQL databases are often used for structured data storage. Tools like PostgreSQL, MySQL, or SQL Server might be used to store and retrieve the data.
- Learn and utilize SQL queries to extract the necessary data.

**3. NoSQL Databases**
- For unstructured or semi-structured data, NoSQL databases like MongoDB, Cassandra, or HBase might be utilized.
- Understand the specific querying method for your NoSQL database of choice.

**4. APIs**
- Data might be gathered from web APIs. This might require understanding of RESTful principles and possibly specific authentication methods.
- Python libraries like `requests` can be used to interact with APIs.

**5. Web Scraping**
- When data is not available through a convenient interface, web scraping might be necessary. Be sure to respect the terms of service of the website and the legality of scraping.
- Python libraries like `BeautifulSoup` or `Scrapy` can be used for web scraping.

**6. File Formats**
- Data might come in various file formats like CSV, Excel, JSON, XML etc. Understand how to parse these formats.
- Python libraries like `pandas` can read many file formats directly into DataFrames.

**7. Cloud Storage**
- Data might be stored on cloud platforms like AWS S3, Google Cloud Storage, or Azure Blob Storage.
- Familiarize yourself with the SDKs of these platforms to interact with the cloud storage (like `boto3` for AWS).

**8. Data Streaming**
- In some cases, data might be continuously generated and streamed. This could be from IoT devices, user interactions on a website, etc.
- Tools like Apache Kafka or Amazon Kinesis could be used to handle this streaming data.

